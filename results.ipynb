{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from models import Models\n",
    "from losses import *\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import scipy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "from time import time, sleep\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = False  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16676190643867356226,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14950894760617235523\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINE OF HORIZONT FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Line_of_horizont_fitting:\n",
    "\n",
    "    line=[]\n",
    "\n",
    "    def inputNormalized(self, image, img_w, img_h):\n",
    "        image=self.resize_image(image, img_w, img_h)\n",
    "        #image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image=image/255.0\n",
    "        return image\n",
    "\n",
    "    def resize_image(self, image, img_w, img_h):\n",
    "        image = cv2.resize(image,(img_w,img_h))\n",
    "        return image\n",
    "\n",
    "    def plot_binary_image(self, image):\n",
    "        image=image*255\n",
    "        binary_img = np.squeeze(image, axis=2)\n",
    "        plt.imshow(binary_img)\n",
    "        plt.show()\n",
    "\n",
    "    def get_binary_image(self, image, treshold):\n",
    "        image = cv2.threshold(image,treshold,1,cv2.THRESH_BINARY)\n",
    "        return image[1]\n",
    "\n",
    "    def binary_edge_detection(self, image):\n",
    "        edges = image - scipy.ndimage.morphology.binary_dilation(image)\n",
    "        return edges\n",
    "\n",
    "    def median_blur(self, img, kernel_size):\n",
    "        \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "        return cv2.blur(img, (kernel_size, kernel_size))\n",
    "\n",
    "    def hough_lines(self, img, rho, theta, threshold, min_line_len, max_line_gap):  \n",
    "        lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "        return lines\n",
    "\n",
    "    def binary2gray(self, image):\n",
    "        image = np.uint8(255 * image)\n",
    "        return image\n",
    "\n",
    "    def Collect_points(self, lines):\n",
    "\n",
    "        # interpolation & collecting points for RANSAC\n",
    "        points=[]\n",
    "        for line in lines:\n",
    "            new_point = np.array([[int(line[0]),int(line[1])]])\n",
    "            points.append(new_point)\n",
    "            new_point = np.array([[int(line[2]),int(line[3])]])\n",
    "            points.append(new_point)\n",
    "\n",
    "        return points\n",
    "\n",
    "    def smoothing(self, lines, pre_frame=10):\n",
    "        # collect frames & print average line\n",
    "        lines = np.squeeze(lines)\n",
    "        avg_line = np.array([0.0,0.0,0.0,0.0])\n",
    "\n",
    "        for ii,line in enumerate(reversed(lines)):\n",
    "            if ii == pre_frame:\n",
    "                break\n",
    "            avg_line += line\n",
    "        avg_line = avg_line / pre_frame\n",
    "\n",
    "        return avg_line\n",
    "\n",
    "    def getLineImage(self, image,label,fit_line, width, height):\n",
    "        height,width,_=image.shape\n",
    "        imageOUT = cv2.bitwise_or(image,label)\n",
    "        cv2.line(imageOUT, (int(fit_line[2]-fit_line[0]*width), int(fit_line[3]-fit_line[1]*width)), (int(fit_line[2]+fit_line[0]*width), int(fit_line[3]+fit_line[1]*width)), (255, 0, 255), 12)\n",
    "\n",
    "    def predict_segmentation(self, image, model):\n",
    "        predict = model.predict(image[None,...])\n",
    "\n",
    "        return predict[0]\n",
    "    \n",
    "    def horizont_line_from_binary_image(self, image, average_n_frame=1, rho = 2, theta = np.pi/180, threshold = 20, min_line_length = 20, \n",
    "                                        max_line_gap = 5):\n",
    "        output = self.binary_edge_detection(image)\n",
    "        output = self.binary2gray(output)\n",
    "\n",
    "        rho = rho# distance resolution in pixels of the Hough grid\n",
    "        theta = theta # angular resolution in radians of the Hough grid\n",
    "        threshold = threshold     # minimum number of votes (intersections in Hough grid cell)\n",
    "        min_line_length = min_line_length #minimum number f pixels making up a line\n",
    "        max_line_gap = 5    # maximum gap in pixels between connectable line segments\n",
    "        lines=self.hough_lines(output, rho, theta, threshold, min_line_length, max_line_gap)\n",
    "        line_arr = np.squeeze(lines)\n",
    "        points = self.Collect_points(line_arr)\n",
    "\n",
    "        if(len(points)<2):\n",
    "            points= line_arr.reshape(lines.shape[0]*2,2)\n",
    "\n",
    "        if(len(points)>2):\n",
    "            fit_line = cv2.fitLine(np.float32(points), cv2.DIST_HUBER, 1, 0.001, 0.001)\n",
    "            #self.line.append(fit_line)\n",
    "\n",
    "            #if len(self.line) > 10:\n",
    "            #    fit_line = self.smoothing(self.line, average_n_frame)\n",
    "\n",
    "        return fit_line\n",
    "        \n",
    "\n",
    "    def horizont_line_pipeline(self, image, model, img_w, img_h, average_n_frame, kernel_median_blur=50, predict_treshold=0.5,\n",
    "                               rho = 2, theta = np.pi/180, threshold = 20, min_line_length = 20, max_line_gap = 5):\n",
    "        or_image=image\n",
    "\n",
    "        or_height, or_width, or_depth = or_image.shape\n",
    "        image=self.inputNormalized(or_image,img_w,img_h)\n",
    "        predict_segmentation = self.predict_segmentation(image,model)\n",
    "        predict = self.median_blur(predict_segmentation,kernel_median_blur)\n",
    "        predict = self.get_binary_image(predict,predict_treshold)\n",
    "        predict = self.resize_image(predict, or_width, or_height)\n",
    "        output = self.binary_edge_detection(predict)\n",
    "        output = self.binary2gray(output)\n",
    "\n",
    "        rho = rho# distance resolution in pixels of the Hough grid\n",
    "        theta = theta # angular resolution in radians of the Hough grid\n",
    "        threshold = threshold     # minimum number of votes (intersections in Hough grid cell)\n",
    "        min_line_length = min_line_length #minimum number f pixels making up a line\n",
    "        max_line_gap = 5    # maximum gap in pixels between connectable line segments\n",
    "        lines=self.hough_lines(output, rho, theta, threshold, min_line_length, max_line_gap)\n",
    "        line_arr = np.squeeze(lines)\n",
    "        points = self.Collect_points(line_arr)\n",
    "\n",
    "        if(len(points)<2):\n",
    "            points= line_arr.reshape(lines.shape[0]*2,2)\n",
    "\n",
    "        if(len(points)>2):\n",
    "            fit_line = cv2.fitLine(np.float32(points), cv2.DIST_HUBER, 1, 0.001, 0.001)\n",
    "            #self.line.append(fit_line)\n",
    "\n",
    "            #if len(self.line) > 10:\n",
    "            #    fit_line = self.smoothing(self.line, average_n_frame)\n",
    "\n",
    "        return fit_line, self.resize_image(predict_segmentation, or_width, or_height)\n",
    "    \n",
    "    def horizont_line_pipeline_verbose(self, image, model, img_w, img_h, average_n_frame, kernel_median_blur=50, predict_treshold=0.5, rho = 2, theta = np.pi/180, threshold = 20, min_line_length = 20, max_line_gap = 5):\n",
    "        or_image=image\n",
    "\n",
    "        or_height, or_width, or_depth = or_image.shape\n",
    "        image=self.inputNormalized(or_image,img_w,img_h)\n",
    "        predict_segmentation = self.predict_segmentation(image,model)\n",
    "        predict = self.median_blur(predict_segmentation,kernel_median_blur)\n",
    "        predict = self.get_binary_image(predict,predict_treshold)\n",
    "        predict = self.resize_image(predict, or_width, or_height)\n",
    "        output = self.binary_edge_detection(predict)\n",
    "        output = self.binary2gray(output)\n",
    "\n",
    "        rho = rho# distance resolution in pixels of the Hough grid\n",
    "        theta = theta # angular resolution in radians of the Hough grid\n",
    "        threshold = threshold     # minimum number of votes (intersections in Hough grid cell)\n",
    "        min_line_length = min_line_length #minimum number f pixels making up a line\n",
    "        max_line_gap = 5    # maximum gap in pixels between connectable line segments\n",
    "        lines=self.hough_lines(output, rho, theta, threshold, min_line_length, max_line_gap)\n",
    "        line_arr = np.squeeze(lines)\n",
    "        points = self.Collect_points(line_arr)\n",
    "\n",
    "        if(len(points)<2):\n",
    "            points= line_arr.reshape(lines.shape[0]*2,2)\n",
    "\n",
    "        if(len(points)>2):\n",
    "            fit_line = cv2.fitLine(np.float32(points), cv2.DIST_HUBER, 1, 0.001, 0.001)\n",
    "            #self.line.append(fit_line)\n",
    "\n",
    "            #if len(self.line) > 10:\n",
    "            #    fit_line = self.smoothing(self.line, average_n_frame)\n",
    "        \n",
    "        pred_visual = predict_segmentation*255\n",
    "        pred_visual= np.uint8(np.concatenate((predict_segmentation,predict_segmentation,pred_visual),axis=2))\n",
    "        return fit_line, self.resize_image(predict_segmentation, or_width, or_height), self.resize_image(or_image, img_w, img_h), self.resize_image(pred_visual, img_w, img_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def median_accuracy_line_of_horizont(x, y, model, inp_w, inp_h, steps=1, visualization=False):\n",
    "        avg_distance=[]\n",
    "        max_distance=[]\n",
    "        \n",
    "        line_of_horizont=Line_of_horizont_fitting()\n",
    "        for label, img in zip(y, x):\n",
    "\n",
    "            image = np.uint8(255 * img)\n",
    "            \n",
    "            label_med=line_of_horizont.median_blur(label,5)\n",
    "            label_med=line_of_horizont.get_binary_image(label_med, 0.5)\n",
    "            fit_line= line_of_horizont.horizont_line_from_binary_image(label_med)\n",
    "\n",
    "            label=line_of_horizont.get_binary_image(label, 0.5)\n",
    "            height,width=label.shape\n",
    "            label_line = np.zeros([height,width], dtype = \"uint8\")\n",
    "            cv2.line(label_line, (int(fit_line[2]-fit_line[0]*width), \n",
    "                                             int(fit_line[3]-fit_line[1]*width)), \n",
    "                     (int(fit_line[2]+fit_line[0]*width), \n",
    "                      int(fit_line[3]+fit_line[1]*width)), (255, 255, 255), 1)\n",
    "\n",
    "            image_pred = line_of_horizont.resize_image(img, inp_w, inp_h)\n",
    "            pred=line_of_horizont.predict_segmentation(image_pred, model)\n",
    "            pred=line_of_horizont.get_binary_image(pred, 0.5)\n",
    "            pred=line_of_horizont.resize_image(pred, width, height)\n",
    "\n",
    "            fit_line, predict=line_of_horizont.horizont_line_pipeline(image, model, inp_w, inp_h, steps, 5)\n",
    "\n",
    "            pred_line = np.zeros([height,width], dtype = \"uint8\")\n",
    "            cv2.line(pred_line, (int(fit_line[2]-fit_line[0]*width), \n",
    "                                             int(fit_line[3]-fit_line[1]*width)), \n",
    "                     (int(fit_line[2]+fit_line[0]*width), \n",
    "                      int(fit_line[3]+fit_line[1]*width)), (255, 255, 255), 1)\n",
    "\n",
    "            distance=[]\n",
    "            for j in range (width):\n",
    "                for i in range (height):\n",
    "                    if(label_line[i,j]==255):\n",
    "                        y1=i\n",
    "                    if(pred_line[i,j]==255):\n",
    "                        y2=i\n",
    "                distance.append(abs(y1-y2))\n",
    "\n",
    "            avg_y= int((y1+y2)/2)\n",
    "            \n",
    "            avg_distance.append(np.mean(distance)/width)\n",
    "            max_distance.append(max(distance))\n",
    "            \n",
    "            if(visualization):\n",
    "                print(\"avg_distance: \", (np.mean(distance)/width),\" - max_distance: \", (max(distance)))\n",
    "                plt.imshow(label_line)\n",
    "                plt.show()\n",
    "                plt.imshow(pred_line)\n",
    "                plt.show()\n",
    "        \n",
    "        return avg_distance, max_distance\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_on_line_of_horizont_area(x, y, model, inp_w, inp_h, steps=1, visualization=False):\n",
    "        recall_list=[]\n",
    "        precision_list=[]\n",
    "        specificity_list=[]\n",
    "        accuracy_list=[]\n",
    "        f1score_list=[]\n",
    "\n",
    "        line_of_horizont=Line_of_horizont_fitting()\n",
    "        for label, img in zip(y, x):\n",
    "\n",
    "            image = np.uint8(255 * img)\n",
    "            label=line_of_horizont.get_binary_image(label, 0.5)\n",
    "            height,width=label.shape\n",
    "\n",
    "            image_pred = line_of_horizont.resize_image(img, inp_w, inp_h)\n",
    "            pred=line_of_horizont.predict_segmentation(image_pred, model)\n",
    "            pred=line_of_horizont.get_binary_image(pred, 0.5)\n",
    "            pred=line_of_horizont.resize_image(pred, width, height)\n",
    "\n",
    "            fit_line, predict=line_of_horizont.horizont_line_pipeline(image, model, inp_w, inp_h, steps)\n",
    "\n",
    "            line_annotation_image = np.zeros([height,width], dtype = \"uint8\")\n",
    "            cv2.line(line_annotation_image, (int(fit_line[2]-fit_line[0]*width), \n",
    "                                             int(fit_line[3]-fit_line[1]*width)), \n",
    "                     (int(fit_line[2]+fit_line[0]*width), \n",
    "                      int(fit_line[3]+fit_line[1]*width)), (255, 255, 255), 1)\n",
    "\n",
    "            for i in range (height):\n",
    "                if(label[i,0]==1):\n",
    "                    y1=i\n",
    "                    break\n",
    "\n",
    "            for i in range (height):\n",
    "                if(label[i,width-1]==1):\n",
    "                    y2=i\n",
    "                    break\n",
    "\n",
    "            avg_y= int((y1+y2)/2)\n",
    "\n",
    "            annotation_image = label[avg_y-100:avg_y+100, 0:width]\n",
    "            pred_image = pred[avg_y-100:avg_y+100, 0:width]\n",
    "\n",
    "            label=annotation_image\n",
    "            pred=pred_image\n",
    "\n",
    "            True_neg=len(np.where((label==0)&(pred==0))[0])\n",
    "            False_neg=len(np.where((label==1)&(pred==0))[0])\n",
    "            True_pos=len(np.where((label==1)&(pred==1))[0])\n",
    "            False_pos=len(np.where((label==0)&(pred==1))[0])\n",
    "            precision=True_pos/(True_pos+False_pos)\n",
    "            recall=True_pos/(True_pos+False_neg)\n",
    "            specificity=1-(True_neg/(True_neg+False_pos))\n",
    "            accuracy=(True_pos+True_neg)/(True_pos+True_neg+False_pos+False_neg)\n",
    "            f1score=2*((precision*recall)/(precision+recall))\n",
    "\n",
    "            recall_list.append(recall)\n",
    "            precision_list.append(precision)\n",
    "            specificity_list.append(specificity)\n",
    "            accuracy_list.append(accuracy)\n",
    "            f1score_list.append(f1score)\n",
    "\n",
    "            if(visualization):\n",
    "                print(\"Recall: \", recall,\" - Precision: \", precision, \" - Specificity: \", specificity, \" - Accuracy: \", \n",
    "                      accuracy, \" - F1score: \", f1score)\n",
    "                plt.imshow(label)\n",
    "                plt.show()\n",
    "                plt.imshow(pred)\n",
    "                plt.show()\n",
    "\n",
    "        return recall_list, precision_list, specificity_list, accuracy_list, f1score_list\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_on_images(x, y, model, inp_w, inp_h, steps=1, visualization=False):\n",
    "        recall_list=[]\n",
    "        precision_list=[]\n",
    "        specificity_list=[]\n",
    "        accuracy_list=[]\n",
    "        f1score_list=[]\n",
    "        \n",
    "        line_of_horizont=Line_of_horizont_fitting()\n",
    "        for label, img in zip(y, x):\n",
    "\n",
    "            label=line_of_horizont.get_binary_image(label, 0.5)\n",
    "            height,width=label.shape\n",
    "            image_pred = line_of_horizont.resize_image(img, 160, 160)\n",
    "            pred=line_of_horizont.predict_segmentation(image_pred, model)\n",
    "            pred=line_of_horizont.get_binary_image(pred, 0.5)\n",
    "            pred=line_of_horizont.resize_image(pred, width, height)\n",
    "\n",
    "            True_neg=len(np.where((label==0)&(pred==0))[0])\n",
    "            False_neg=len(np.where((label==1)&(pred==0))[0])\n",
    "            True_pos=len(np.where((label==1)&(pred==1))[0])\n",
    "            False_pos=len(np.where((label==0)&(pred==1))[0])\n",
    "            precision=True_pos/(True_pos+False_pos)\n",
    "            recall=True_pos/(True_pos+False_neg)\n",
    "            specificity=1-(True_neg/(True_neg+False_pos))\n",
    "            accuracy=(True_pos+True_neg)/(True_pos+True_neg+False_pos+False_neg)\n",
    "            f1score=2*((precision*recall)/(precision+recall))\n",
    "            \n",
    "            recall_list.append(recall)\n",
    "            precision_list.append(precision)\n",
    "            specificity_list.append(specificity)\n",
    "            accuracy_list.append(accuracy)\n",
    "            f1score_list.append(f1score)\n",
    "            \n",
    "            if(visualization):\n",
    "                print(\"Recall: \", recall,\" - Precision: \", precision, \" - Specificity: \", specificity, \" - Accuracy: \", \n",
    "                      accuracy, \" - F1score: \", f1score)\n",
    "                plt.imshow(label)\n",
    "                plt.show()\n",
    "                plt.imshow(pred)\n",
    "                plt.show()\n",
    "            \n",
    "        return recall_list, precision_list, specificity_list, accuracy_list, f1score_list\n",
    "            \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def test_speed_from_video(filename, model, inp_w, inp_h, n_iteration, steps=1):\n",
    "        lineofhorizont = Line_of_horizont_fitting()\n",
    "        reader = imageio.get_reader(filename,  'ffmpeg')\n",
    "        fps = reader.get_meta_data()['fps']\n",
    "        n_steps=0\n",
    "        frame_to_discard = 10\n",
    "        now=time()\n",
    "        for i in range(n_iteration):\n",
    "            if i == frame_to_discard:\n",
    "                start_time=now\n",
    "            if i > frame_to_discard:\n",
    "                #n_steps+=1\n",
    "                elapsed_time = now - start_time\n",
    "                #print(n_steps, elapsed_time)\n",
    "            or_image=reader.get_data(i)\n",
    "            or_height, or_width, or_depth = or_image.shape\n",
    "\n",
    "            fit_line, predict=lineofhorizont.horizont_line_pipeline(or_image, model, inp_w, inp_h, steps)\n",
    "\n",
    "            predict = predict.reshape(or_height,or_width,1)\n",
    "            predict1 = predict*255\n",
    "            predict= np.uint8(np.concatenate((predict,predict,predict1),axis=2))\n",
    "            imageOUT = cv2.bitwise_or(or_image,predict)\n",
    "            imageOUT=cv2.line(imageOUT, (int(fit_line[2]-fit_line[0]*or_height), int(fit_line[3]-fit_line[1]*or_width)), \n",
    "                              (int(fit_line[2]+fit_line[0]*or_height), int(fit_line[3]+fit_line[1]*or_width)), \n",
    "                              (255, 0, 255), 5)\n",
    "            now=time()\n",
    "            \n",
    "        reader.close()\n",
    "        return ((i-frame_to_discard))/elapsed_time\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_speed_from_video_v2(reader, model, inp_w, inp_h, n_iteration, steps=1):\n",
    "        n_steps=0\n",
    "        frame_to_discard = 10\n",
    "        now=time()\n",
    "        lineofhorizont = Line_of_horizont_fitting()\n",
    "        for i in range(n_iteration):\n",
    "            if i == frame_to_discard:\n",
    "                start_time=now\n",
    "            if i > frame_to_discard:\n",
    "                #n_steps+=1\n",
    "                elapsed_time = now - start_time\n",
    "                #print(n_steps, elapsed_time)\n",
    "            or_image=reader.get_data(i)\n",
    "            or_height, or_width, or_depth = or_image.shape\n",
    "\n",
    "            fit_line, predict=lineofhorizont.horizont_line_pipeline(or_image, model, inp_w, inp_h, steps)\n",
    "\n",
    "            predict = predict.reshape(or_height,or_width,1)\n",
    "            predict1 = predict*255\n",
    "            predict= np.uint8(np.concatenate((predict,predict,predict1),axis=2))\n",
    "            imageOUT = cv2.bitwise_or(or_image,predict)\n",
    "            imageOUT=cv2.line(imageOUT, (int(fit_line[2]-fit_line[0]*or_height), int(fit_line[3]-fit_line[1]*or_width)), \n",
    "                              (int(fit_line[2]+fit_line[0]*or_height), int(fit_line[3]+fit_line[1]*or_width)), \n",
    "                              (255, 0, 255), 5)\n",
    "            now=time()\n",
    "\n",
    "        return ((i-frame_to_discard))/elapsed_time\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_from_video(filename, model, inp_w, inp_h, n_iteration, steps=1):\n",
    "        lineofhorizont = Line_of_horizont_fitting()\n",
    "        reader = imageio.get_reader(filename,  'ffmpeg')\n",
    "        fps = reader.get_meta_data()['fps']\n",
    "        now=time()\n",
    "        start_time=now\n",
    "        for i in range(n_iteration):\n",
    "            elapsed_time = now - start_time\n",
    "            print(i, elapsed_time)\n",
    "            or_image=reader.get_data(i)\n",
    "            plt.imshow(or_image)\n",
    "            plt.show()\n",
    "            or_height, or_width, or_depth = or_image.shape\n",
    "\n",
    "            fit_line, predict=lineofhorizont.horizont_line_pipeline(or_image, model, inp_w, inp_h, steps)\n",
    "\n",
    "            predict = predict.reshape(or_height,or_width,1)\n",
    "            predict1 = predict*255\n",
    "            predict= np.uint8(np.concatenate((predict,predict,predict1),axis=2))\n",
    "            imageOUT = cv2.bitwise_or(or_image,predict)\n",
    "            imageOUT=cv2.line(imageOUT, (int(fit_line[2]-fit_line[0]*or_height), int(fit_line[3]-fit_line[1]*or_width)), \n",
    "                              (int(fit_line[2]+fit_line[0]*or_height), int(fit_line[3]+fit_line[1]*or_width)), \n",
    "                              (255, 0, 255), 5)\n",
    "            now=time()\n",
    "            plt.imshow(imageOUT)\n",
    "            plt.show()\n",
    "        reader.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def test_from_folder(path, model, inp_w, inp_h, steps=1):\n",
    "        lineofhorizont = Line_of_horizont_fitting()\n",
    "        path_images=glob.glob(path)\n",
    "        images=[]\n",
    "        now=time()\n",
    "        start_time=now\n",
    "        for path_img in path_images:\n",
    "            elapsed_time = now - start_time\n",
    "            print(elapsed_time)\n",
    "            or_image=cv2.imread(path_img)\n",
    "            or_image=cv2.cvtColor(or_image, cv2.COLOR_BGR2RGB)\n",
    "            or_height, or_width, or_depth = or_image.shape\n",
    "\n",
    "            fit_line, predict, img_inp_or, pred_inp_or=lineofhorizont.horizont_line_pipeline_verbose(or_image, model, inp_w, inp_h, steps)\n",
    "\n",
    "            predict = predict.reshape(or_height,or_width,1)\n",
    "            predict1 = predict*255\n",
    "            predict= np.uint8(np.concatenate((predict,predict,predict1),axis=2))\n",
    "            imageOUT = cv2.bitwise_or(or_image,predict)\n",
    "            \n",
    "            #(x0-m*vx[0], y0-m*vy[0]), (x0+m*vx[0], y0+m*vy[0])\n",
    "            \n",
    "            W = or_width \n",
    "            H = or_height\n",
    "            \n",
    "            x0 = (int(fit_line[2]-(W*fit_line[0])))\n",
    "            x1 = (int(fit_line[2]+(W*fit_line[0])))\n",
    "            y0 = (int(fit_line[3]-(H*fit_line[1])))\n",
    "            y1 = (int(fit_line[3]+(H*fit_line[1])))\n",
    "            \n",
    "            imageOUT=cv2.line(imageOUT, (x0,y0), (x1,y1), (255, 0, 255), 5)\n",
    "            now=time()\n",
    "        \n",
    "            yield path_img, imageOUT, predict, img_inp_or, pred_inp_or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8e80b69d9529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#model1=model_class.load_model(path_models+'unet160--mediumBN--dice_coef_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PINV15177/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Carga el modelo guardado de acuerdo a las perdidas(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;31m# TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m       loader = loader_cls(object_graph_proto,\n\u001b[0m\u001b[1;32m    603\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                           export_dir)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WrapperFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# loaded from config may create variables / other objects during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# initialization. These are recorded in `_nodes_recreated_from_config`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Load all other nodes and functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layer\u001b[0;34m(self, proto, node_id)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_revive_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevive_custom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;31m# Add an attribute that stores the extra functions/objects saved in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mrevive_custom_object\u001b[0;34m(identifier, metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrevived_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m     raise ValueError('Unable to restore custom object of type {} currently. '\n\u001b[0m\u001b[1;32m    779\u001b[0m                      \u001b[0;34m'Please make sure that the layer implements `get_config`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                      \u001b[0;34m'and `from_config` when saving. In addition, please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`."
     ]
    }
   ],
   "source": [
    "path_models='./models_saved/unet160--largeMobileNet--dice_coef_loss'\n",
    "img_w=160\n",
    "img_h=160\n",
    "# Para custom objects\n",
    "def f1(y_true, y_pred):\n",
    "    return 1\n",
    "\n",
    "model_class = Models()\n",
    "\n",
    "\n",
    "model1 = model_class.load_model(path_models)\n",
    "\n",
    "#model1=model_class.load_model(path_models+'unet160--mediumBN--dice_coef_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a030dc34ba2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./video_input/Test2/*\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_2_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./video_input/Test2_result/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpath_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_inp_or\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_inp_or\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpath_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "path_images = \"./video_input/Test2/*\"\n",
    "path_2_save = \"./video_input/Test2_result/\"\n",
    "for path_img, imageOUT, predict, img_inp_or, pred_inp_or in Utils.test_from_folder(path_images, model1, img_w, img_h, steps=300):\n",
    "    name= path_img[path_img.rfind(\"/\") + 1::]\n",
    "    print(name)\n",
    "    cv2.imwrite(path_2_save+\"prediction-\"+name, cv2.cvtColor(imageOUT, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(path_2_save+\"mask_prediction-\"+name, cv2.cvtColor(predict, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(path_2_save+\"image_inpNN_or_size-\"+name, cv2.cvtColor(img_inp_or, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(path_2_save+\"mask_prediction_or_size-\"+name, cv2.cvtColor(pred_inp_or, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"video_input/prophr3.mp4\"\n",
    "\n",
    "Utils.test_from_video(filename, model1, img_w, img_h, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"video_input/lakegarda-may-9-prop-8.avi\"#prophr3.mp4\"\n",
    "\n",
    "Utils.test_speed_from_video(filename, model1, img_w, img_h, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "path = 'Test/'\n",
    "img_w = 1280\n",
    "img_h = 720\n",
    "n_labels = 2 #6\n",
    "dataset = Dataset(path, img_w, img_h, n_labels)\n",
    "x, y = dataset.createDataset(augmentation=False, prob1=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = Models()\n",
    "path_models='./models_saved/Final_2/'\n",
    "model_path_name= ['unet160--small--dice_coef_loss',\n",
    "                  'unet160--smallBN--dice_coef_loss',\n",
    "                  'unet160--medium--dice_coef_loss',\n",
    "                  'unet160--mediumBN--dice_coef_loss',\n",
    "                  'unet160--mediumBN_mobilenetv1--dice_coef_loss',\n",
    "                  ]\n",
    "\n",
    "for models_name in model_path_name:\n",
    "    model1=model_class.load_model(path_models+models_name)\n",
    "    recall_list, precision_list, specificity_list, accuracy_list, f1score_list = Utils.accuracy_on_images(x, y, model1, 160, 160, 1, False)\n",
    "    print('*' * 30)\n",
    "    print(models_name)\n",
    "    print('N PARAMETERS =', model1.count_params())\n",
    "    print(\"RECALL =\", sum(recall_list) / float(len(recall_list)))\n",
    "    print(\"PRECISION =\", sum(precision_list) / float(len(precision_list)))\n",
    "    print(\"SPECIFICITY =\", sum(specificity_list) / float(len(specificity_list)))\n",
    "    print(\"ACCURACY =\", sum(accuracy_list) / float(len(accuracy_list)))\n",
    "    print(\"F1SCORE =\", sum(f1score_list) / float(len(f1score_list)))\n",
    "    print('*' * 30)\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = Models()\n",
    "path_models='./models_saved/Final_2/'\n",
    "model_path_name= ['unet160--small--dice_coef_loss',\n",
    "                  'unet160--smallBN--dice_coef_loss',\n",
    "                  'unet160--medium--dice_coef_loss',\n",
    "                  'unet160--mediumBN--dice_coef_loss',\n",
    "                  'unet160--mediumBN_mobilenetv1--dice_coef_loss',\n",
    "                  ]\n",
    "\n",
    "for models_name in model_path_name:\n",
    "    model1=model_class.load_model(path_models+models_name)\n",
    "    recall_list, precision_list, specificity_list, accuracy_list, f1score_list = Utils.accuracy_on_line_of_horizont_area(x, y, model1, 160, 160, 1, True)\n",
    "    print('*' * 30)\n",
    "    print(models_name)\n",
    "    print('N PARAMETERS =', model1.count_params())\n",
    "    print(\"PRECISION =\", sum(precision_list) / float(len(precision_list)))\n",
    "    print(\"RECALL =\", sum(recall_list) / float(len(recall_list)))\n",
    "    #print(\"SPECIFICITY =\", sum(specificity_list) / float(len(specificity_list)))\n",
    "    print(\"ACCURACY =\", sum(accuracy_list) / float(len(accuracy_list)))\n",
    "    print(\"F1SCORE =\", sum(f1score_list) / float(len(f1score_list)))\n",
    "    print('*' * 30)\n",
    "\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = Models()\n",
    "path_models='./models_saved/Final_2/'\n",
    "model_path_name= ['unet160--small--dice_coef_loss',\n",
    "                  'unet160--smallBN--dice_coef_loss',\n",
    "                  'unet160--medium--dice_coef_loss',\n",
    "                  'unet160--mediumBN--dice_coef_loss',\n",
    "                  'unet160--mediumBN_mobilenetv1--dice_coef_loss',\n",
    "                  ]\n",
    "\n",
    "max_distances_array = []\n",
    "names_array = []\n",
    "\n",
    "for models_name in model_path_name:\n",
    "    model1=model_class.load_model(path_models+models_name)\n",
    "    avg_distance, max_distance = Utils.median_accuracy_line_of_horizont(x, y, model1, 160, 160, 1, False)\n",
    "    print('*' * 30)\n",
    "    print(\"-- \", models_name, \" --\")\n",
    "    print(\"Mean Max error :\", np.mean(max_distance))\n",
    "    print(\"Median Max error :\", np.median(max_distance))\n",
    "    print(\"Standard Deviation :\", np.std(max_distance))\n",
    "    print(\"Quantile 0.25-0.75:\", np.quantile(max_distance, 0.25), \" - \", np.quantile(max_distance, 0.75))\n",
    "    print(\"Average error :\", np.mean(avg_distance))\n",
    "    print(\"Max error :\", np.max(max_distance))\n",
    "    print('*' * 30)\n",
    "    \n",
    "    names_array.append(models_name)\n",
    "    max_distances_array.append(max_distance)\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_array = [\"Half-Conv 160x160\", \"Half-Conv BN 160x160\", \"Full 160x160\", \"Full BN 160x160\", \"Full mobile-net-v1-layer 160x160\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, (ax1, ax2) = plt.subplots(1, 2, sharey=True, gridspec_kw={'width_ratios': [10, 10]})\n",
    "\n",
    "ax1.boxplot(max_distances_array, vert=False, labels=names_array)\n",
    "ax1.set_xlim(0,30)\n",
    "\n",
    "ax2.boxplot(max_distances_array, vert=False, labels=names_array)\n",
    "ax2.set_xlim(60,100)\n",
    "\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax1.yaxis.tick_left()\n",
    "ax1.tick_params(labelright='off')\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.tick_params(labelright='off')\n",
    "\n",
    "d = .015 # how big to make the diagonal lines in axes coordinates\n",
    "# arguments to pass plot, just so we don't keep repeating them\n",
    "kwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\n",
    "ax1.plot((1-d,1+d), (-d,+d), **kwargs)\n",
    "ax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\n",
    "\n",
    "kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "ax2.plot((-d,+d), (1-d,1+d), **kwargs)\n",
    "ax2.plot((-d,+d), (-d,+d), **kwargs)\n",
    "\n",
    "fig1.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig2, ax2 = plt.boxplot(max_distances_array, vert=False, labels=names_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd models_saved/history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "history_path=['models_saved/history/unet160--history--largeBN--binary_crossentropy',\n",
    "              'models_saved/history/unet160--history--largeBN--mae', \n",
    "              'models_saved/history/unet160--history--largeBN--mse',\n",
    "              'models_saved/history/unet160--history--largeBN--dice_coef_loss', ]\n",
    "\n",
    "historyes = []\n",
    "for path in history_path:\n",
    "    with (open(path+\"\", \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                historyes.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(historyes[0].keys())\n",
    "# summarize history for loss\n",
    "for history in historyes:\n",
    "    plt.plot(history['val_dice_coef'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Dice/F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['binary_crossentropy_loss',\n",
    "            'mean_absolute_error_loss',\n",
    "            'mean_squared_error_loss',\n",
    "            'dice_coef_loss'], loc='lower right')\n",
    "plt.axis([0,200,0.9,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "file = \"ter1_1-annotation.png\"\n",
    "img=cv2.imread(file,0)\n",
    "hist = np.histogram(img.flatten(),256,[0,256])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 917031 1156569       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0]\n"
     ]
    }
   ],
   "source": [
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
